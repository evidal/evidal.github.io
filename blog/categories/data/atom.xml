<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: data | Le Blog d'Eric Vidal]]></title>
  <link href="http://evidal.github.io/blog/categories/data/atom.xml" rel="self"/>
  <link href="http://evidal.github.io/"/>
  <updated>2015-01-12T22:04:24+01:00</updated>
  <id>http://evidal.github.io/</id>
  <author>
    <name><![CDATA[Eric Vidal]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[MOOC Machine Learning par Andrew Ng]]></title>
    <link href="http://evidal.github.io/blog/2014/12/23/mooc-machine-learning/"/>
    <updated>2014-12-23T00:00:00+01:00</updated>
    <id>http://evidal.github.io/blog/2014/12/23/mooc-machine-learning</id>
    <content type="html"><![CDATA[<p>Ca y'est, c'est fait. Je viens de terminer le MOOC <a href="https://www.coursera.org/course/ml">Machine Learning</a> donnée par <a href="http://cs.stanford.edu/people/ang/">Andrew Ng</a> de l'université de Stanford. Je suis d'autant plus content que ce MOOC, est assez conséquent. Il dure 10 semaines avec environ 5-7 heures de travail par semaine. J'avais déjà fait 2 tentatives sur un autre sujet, mais j'avais arrêté au premier tiers à chaque fois par manque de temps. En effet toutes les semaines il faut rendre un exercice de programmation et répondre à un ou plusieurs QCM. Si on rend ses devoirs en retard, on est pénalisé de 20%. Et pour être &ldquo;dîplomé&rdquo;, il faut obtenir au moins 80% de réussite. Bref, si vous voulez suivre un MOOC, il faut se dégager du temps et être régulier dans son travail, sous peine de perdre le fil.</p>

<p>Le cours balayait les méthodes de base du machine learning avec à chaque fois une application pratique:</p>

<ul>
<li><strong>régression linéaire</strong>, pour prédire une variable en fonction de plusieurs entrées</li>
<li>rappel d'<strong>algèbre linéaire</strong>, pour revoir toutes les maths associées aux matrices et aux vecteurs</li>
<li><strong>régression linaire</strong> avec plusieurs variables, pour predire plusieurs variables en fonction de plusieurs entrées</li>
<li><strong>régression logistique</strong>, classification sur une ou plusieurs classes et normalisation des données (avec en application un OCR !)</li>
<li><strong>réseau neuronaux</strong> (toujours l'OCR en application)</li>
<li><strong>conseils pratiques</strong> pour la mise en oeuvre des algorithmes de Machine Learning : Comment développer, débugger, structurer, tester, etc&hellip;</li>
<li><strong>Machine à vecteurs de support (SVMs)</strong>, un algorithme encore plus puissant de classification</li>
<li>Apprentissage non supervisé: <strong>clustering</strong>, pour former des groupes d'éléments se ressemblant</li>
<li><strong>Réduction dimensionnelle</strong> pour notamment la visualisation des données</li>
<li><strong>Détection d'anomalies</strong></li>
<li><strong>Système de recommandations</strong> qui est en fait le résultat de plusieurs classifications faite à grand échelle</li>
<li>Machine Learning sur des <strong>gros volumes de données</strong> (Big Data)</li>
<li>Présentation d'une application complète de Machine Learning (plusieurs algorithmes enchainés pour effectuer une tâche)</li>
</ul>


<p>Au niveau de la plate-forme, <a href="https://www.coursera.org">Coursera</a> est un site très bien fait. La recherche de cours est efficace et les sujets sont assez variés. Les cours sont présentés à travers de petites vidéos d'introduction et d'un résumé des sujets abordés.</p>

<p>Une fois inscrit à un cours, on accède à une page dédiée à celui-ci. Cette page est mise à jour toutes les semaines avec l'ajout des vidéos du cours de la semaine et parfois de news. Ces vidéos montrent le professeur et un support de cours. Le professeur dispose d'un crayon avec lequel il peut annoter le support du cours, ce qui rend sa présentation plus vivante. De plus les vidéos sont parfois agrémentées de QCM (ne comptant pas dans la note finale) permettant de vérifier que l'on comprend bien le cours. Les vidéos et les supports sont disponibles en téléchargement pour une utilisation off-line, ce qui est pratique quand on est en déplacement.</p>

<p>Chaque semaine des devoirs doivent être rendu. Que ce soit un QCM ou un exercice de programmation, les résultats sont connus immédiatement. On peut soumettre un devoir autant de fois que l'on veut, ce qui permet de viser à chaque fois un score parfait. Des dispositifs sont en place pour éviter la soumission de devoir en force brute, ainsi chaque tentative de QCM doit être espacée d'au moins 10 minutes et de 5 minutes pour les exercices de programmation.</p>

<p>Les exercices de programmation utilisait <a href="https://www.gnu.org/software/octave/">Octave</a> une version GNU de <a href="http://fr.mathworks.com/products/matlab/index.html?ref=nn_matlab">Matlab</a>. Chaque semaine il fallait télécharger une archive contenant un code source incomplet et des instructions en PDF pour le compléter. Le code source contenait également de quoi soumettre l'exercice en ligne.</p>

<p>Enfin un forum et un chanel IRC était disponible pour socialiser pendant le cours ou poser des questions à l'équipe pédagogique.</p>

<p>Bref que ce soit au niveau contenu ou niveau de la forme, c'était un vrai plaisir de suivre ce cours et une fierté d'être allé au bout.
Je vous conseille vivement de participer à une des prochaines sessions.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction &agrave; Hadoop]]></title>
    <link href="http://evidal.github.io/blog/2014/06/23/hadoop-big-data/"/>
    <updated>2014-06-23T00:00:00+02:00</updated>
    <id>http://evidal.github.io/blog/2014/06/23/hadoop-big-data</id>
    <content type="html"><![CDATA[<p><a href="http://hadoop.apache.org/">Hadoop</a> est un système permettant de gérer des très gros volumes de données aussi bien au niveau de leur
stockage qu'au niveau de leur traitement. Pour mettre les choses au clair tout de suite, Hadoop est une solution à oublier en
dessous d'une grosse dizaine de serveurs. Hadoop est un système taillé pour gérer des centaines voir des milliers de serveurs. Il faut donc dégainer cette solution quand c'est nécéssaire. Google utilise par exemple ce type de technologie pour gérer les gros
traitements sur son cluster de serveurs. C'est d'ailleurs Google qui est à l'origine des concepts fondateurs d'Hadoop.</p>

<!--more-->


<p>Hadoop n'est pas monobloc, c'est plutot un ensemble de composants spécialisés.</p>

<h1>HDFS</h1>

<p>Hadoop Distributed File System (HDFS) est l'implémentation open source des concepts introduits par le Google File System.
C'est un système de fichier ditribué, taillé pour les gros volumes de données. HDFS est capable de gérer des volumes de
plusieurs péta octets. Les blocs de données sont proportionnelles à la volumétrie potentielle et sont fixés par défaut à
64 méga octets.</p>

<p><img src="/images/posts/2014-06-15-hadoop-big-data/hdfsarchitecture.gif" alt="HDFS Architecture" />
(source <a href="http://hadoop.apache.org/docs/r1.2.1/hdfs_design.html">http://hadoop.apache.org/docs/r1.2.1/hdfs_design.html</a>)</p>

<p>Le Namenode est le serveur du cluster Hadoop s'occupant des méta-données du file system (Nom, réplication, emplacement, etc&hellip;)</p>

<p>Les serveurs de données eux stockent les blocs des fichiers.</p>

<h1>Map Reduce</h1>

<p>Map Reduce est un modèle de programmation permettant de distribuer l'éxécution d'une tache sur plusieurs serveurs (e.g. un tri, une
indexation, un calcul, etc.) Le &ldquo;travail&rdquo; de Map Reduce va être de découper, trier et rassembler les données à différents moment de
l'éxécution d'une tâche.</p>

<p>On peut schématiser l'éxection d'un batch Map Reduce avec le schéma suivant:</p>

<p><img src="/images/posts/2014-06-15-hadoop-big-data/MapReduce.png" alt="HDFS Architecture" /></p>

<ul>
<li>Les données stockées dans des blocs HDFS vont être tronçonnées en InputSplit.

<ul>
<li>e.g. des lignes d'un fichier de log</li>
</ul>
</li>
<li>Les records readers vont assigner une clé k1 et une valeur v1 pour toute donnée lue.

<ul>
<li>e.g. le numéro de la ligne comme clé, la ligne compléte en valeur</li>
</ul>
</li>
<li>Le fonction de Mapping va lire et transformer cette première entrée en une autre paire clé-valeur (k2, v2).

<ul>
<li>e.g. La ligne de log est analysée, on met en clé le nom d'un utilisateur et en valeur 1, représentant un login par exemple</li>
</ul>
</li>
<li>La fonction de partition va déterminer en fonction de la clé k2 vers quel Reducer la clé va être routée</li>
<li>La fonction de Shuffle and Sort rassemble par clé k2 toutes les valeurs v2 émises pas les Mappers

<ul>
<li>e.g. On se retrouve à cette étape avec le nom d'un utilisateur et une liste de 1 correspondant à toutes les occurences de connections</li>
</ul>
</li>
<li>Le Reducer va transformer une clé k2 et sa liste de valeur v2 en une nouvelle clé k3 et une nouvelle valeur v3.

<ul>
<li>e.g. Le reducer va sommer toutes les valeurs pour un utilisateur en comptant toutes les valeurs de la liste. Par exemple evidal c'est connecté 3 fois.</li>
</ul>
</li>
<li>Le traitement étant terminé, on peut procéder à l'écriture des fichiers.</li>
</ul>


<p>Implémentez un job Map Reduce pour Hadoop consiste en l'assemblage de Mapper, Reducer, Reader, Writer, Partionner.</p>

<p>Des implémentations de certaines de ces classes sont déjà fournies pour des tâches simples (e.g. identités).</p>

<p>L'assemblage se fait via une une classe Java qui va positionner via des setters les éléménts à utiliser.
On livre sur le cluster un jar contenant les classes nécéssaires à l'éxécution d'un job.</p>

<p>Pour l'éxécution, Hadoop va instancier une JVM sur les différents noeuds et éxécuter le job qu'on a livré.<br/>
En développement, un mode classique (lancé de manière transparente) permet d'éxécuter un job Map Reduce en dehors d'un cluster Hadoop.<br/>
Pour les test unitaire, on peut utiliser des outils comme <a href="http://mrunit.apache.org/">MRUnit</a>.</p>

<h1>Hive, Impala, Pig &amp; Oozie</h1>

<p>Map Reduce, c'est bien mais ça peut devenir un peu fastidieux s'il faut écrire des jointures. D'autant plus que ce n'est pas vraiment passionnant à coder. Des outils de plus haut niveau ont été développés pour travailler plus facilement qu'avec Map Reduce.</p>

<ul>
<li><a href="http://hive.apache.org/">Hive</a> est un langage très similaire au SQL. Tout d'abord on mappe les données d'un fichier pour exposer son contenu comme une table. Une fois que l'on a déclaré plusieurs tables, on peut écrire des requêtes pour interroger le cluster Hadoop comme un base de données relationnelle. Ca semble trivial, mais il faut toujours penser que ce système est fait pour gérer de très gros volumes de données, pas des volumétries standards.
<code>
SELECT stock.product, SUM(orders.purchases)
FROM stock JOIN orders
ON (stock.id = orders.stock_id)
WHERE orders.quarter = 'Q1'
GROUP BY stock.product;
</code></li>
<li><a href="http://pig.apache.org/">Pig</a> est un autre système permettant d'écrire des jobs Map Reduce de manière plus simple. Pig prend plus la forme d'un script, on indique quelles données charger et les opérations à faire avec.
<code>
stock = LOAD '/user/fred/stock' AS (id, item);
orders = LOAD '/user/fred/orders' AS (id, cost);
grpd = GROUP orders BY id;
totals = FOREACH grpd GENERATE group,
SUM(orders.cost) AS t;
result = JOIN stock BY id, totals BY group;
DUMP result;
</code></li>
<li><a href="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html">Impala</a> ressemble à Hive du point de vue syntaxique. La grosse différence entre les 2 est qu'Impala ne va pas générer un job Map Reduce, mais va utiliser d'autres algorithmes pour éxécuter la requête et donner une réponse très rapidement.</li>
</ul>


<p>Oozie est un système permettant d'écrire plusieurs tâches qui vont s'enchainer. Par exemlple commencer par un job Hive, suivi d'un job Map Reduce classique et enfin d'un autre job Hive. Cela permet de créer des chaines de traitement plus complexes.</p>

<h1>Distribution</h1>

<p>Si Hadoop est disponible sur le site du projet Apache, des distributions sont également disponibles chez des éditeurs:</p>

<ul>
<li><a href="http://www.cloudera.com/content/cloudera/en/home.html">Cloudera</a></li>
<li><a href="Horton%20Works">Horton Works</a></li>
<li><a href="http://www.mapr.com/">MapR</a></li>
</ul>


<p>Le gros avantage de passer par une distribution commerciale est que l'on peut acheter du support. Parfois, ces distributions assemblent des versions différentes des composants core pour avoir un fonctionnement plus cohérent.</p>

<p>Enfin, sur leurs sites, des machines virtuelles (e.g. chez <a href="http://www.cloudera.com/content/support/en/downloads/quickstart_vms/cdh-5-0-x.html">Cloudera</a>) sont disponibles pour tester, ce qui est un gros gain de temps.</p>

<h1>Conclusion</h1>

<p>Hadoop n'est pas la solution que l'on va sortir à tout bout de champ. Le but est de manipuler de très très gros volumes de données.
C'est clairement un outil qui va devenir de plus en plus utile avec l'explosion des données récoltées par les objets connectées ou toutes les traces que nous laissons sur Internet.</p>
]]></content>
  </entry>
  
</feed>
